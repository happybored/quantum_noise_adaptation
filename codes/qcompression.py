# -*- coding: utf-8 -*-
"""QCompression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Xe1YS9_Ut7pZi7gTwMjq2BoQXLdUzgKX

## Tutorial *2.5*: QNN Compression

### Tutorial *2.5.1*:  LUT Construction

#### Setup
"""

# ! pip install qiskit
# ! pip install pandas
# ! pip install torchquantum
# # ! python -m pip uninstall matplotlib
# ! pip install matplotlib==3.1.3
# !wget https://www.dropbox.com/s/qthhn8ispg631v2/model.pth

from qiskit import transpile 
from qiskit import QuantumCircuit
import math
import sys
from qiskit.test.mock import FakeValencia
import pandas as pd
import random
import torch
import numpy as np
import os
import torchquantum as tq
import torchquantum.functional as tqf
from torch.utils.data import Dataset,DataLoader
import numpy as np
import pandas as pd
import torch.nn.functional as F
import torch
import torch
import torch.nn as nn
from torch.nn.parameter import Parameter

import torchquantum as tq
import torchquantum.functional as tqf
from torch.utils.data import Dataset,DataLoader
import numpy as np
import pandas as pd
import torch.nn.functional as F
import torch
from qiskit.compiler.transpiler import transpile
from torchquantum.plugins import tq2qiskit, qiskit2tq
from torch.nn.parameter import Parameter

"""Tutori### LUT Construction"""

def LUT_construction(fixing_points,logical_gates, backend):
    head = ["fixing_points"]
    head.extend(logical_gates)
    df = pd.DataFrame(columns=head)
    for val in fixing_points:
        row = []
        row.append("{:.2f}".format(val))
        for gate in logical_gates:
            if gate in ['rx','ry','rz']:
                circ = QuantumCircuit(1, 1)
                eval('circ.{}(val,0)'.format(gate))
            if gate in ['crx','cry','crz']:
                circ = QuantumCircuit(2, 2)
                eval('circ.{}(val,0,1)'.format(gate))
            transpiled_circ = transpile(circ, backend=backend, optimization_level=2, seed_transpiler=0)
            depth = transpiled_circ.depth()
            row.append(depth)
        df.loc[len(df.index)] = row
    return df

def get_weight_from_model(model):
    W =  list(model.parameters())
    W = torch.tensor(W)
    return W

def set_model_weight(model,W):
    i = 0
    for para in model.parameters():
        para.data = Parameter(torch.tensor([[W[i].data]]))
        i = i+1

def get_fixing_points_from_lut(lut):
  gates_fixing_points = list(lut['fixing_points'][:-1])
  return gates_fixing_points,len(lut)-1

def get_model_depth(model,backend):
  circ = tq2qiskit(tq.QuantumDevice(n_wires=model.n_wires), model.q_layer, draw=True)
  transpiled_circ = transpile(circ,backend=backend,seed_transpiler=0)
  return transpiled_circ.depth()
  
def LUT_reconstrution(model,lut,backend,metrics_func,test_func,dataflow):
  q_model = model.q_layer
  W = get_weight_from_model(q_model)
  circ = tq2qiskit(tq.QuantumDevice(n_wires=model.n_wires), q_model, draw=True)
  transpiled_circ = transpile(circ,backend=backend,seed_transpiler=0)
  original_depth = transpiled_circ.depth()
  original_acc = test_func(model,dataflow)

  gates_fixing_points,max_len = get_fixing_points_from_lut(lut) 
  para_acc = np.zeros([W.shape[0],max_len])
  para_depth = np.zeros_like(para_acc)
  para_metrics1 = np.zeros_like(para_acc)
  best_fixing_points =np.zeros(W.shape[0])
  print(W.shape[0],max_len)
  for i in range(W.shape[0]):
    for j in range(max_len):
      W2 = W.clone()
      W2[i]= gates_fixing_points[j]
      set_model_weight(model,W2)
      acc2 = test_func(model,dataflow)
      depth2 = get_model_depth(model.q_layer,backend=backend)
      para_acc[i][j] = acc2 *1.0 /original_acc
      para_depth[i][j] = depth2 *1.0/original_depth
  para_metrics1 = metrics_func(para_acc,para_depth)
  best_index = np.argmax(para_metrics1,axis=1)
  for i in range(W.shape[0]):
    best_fixing_points[i] = gates_fixing_points[best_index[i]]
  return best_fixing_points


"""#### Class: ADMM"""

def get_fixing_parameters(weight,regu_val = 6.2831853071796 ):
    fixing_paras = torch.zeros_like(weight)
    return fixing_paras





class ADMM:
    def __init__(self, model,args,rho=0.001,device = torch.device("cuda" if torch.cuda.is_available() else "cpu")):
        self.ADMM_U = {}
        self.ADMM_Z = {}
        self.rho = rho
        self.args =args
        self.device = device
        self.init(model,args.prune_ratio)


    def get_fixing_abs(self,data,fix_para,weight=None):
        fixing_abs= torch.abs(data-fix_para) 
        if weight != None:
            for i in range(len(data)):
                fixing_abs[i] = fixing_abs[i]/weight[self.wires[i]]
        return fixing_abs
    
    def get_weight_from_model(self,model):
        W =  list(model.parameters())
        W = torch.tensor(W).to(self.device)
        return W
    def get_wires_name_from_model(self,model):
        wires = list()
        for op in model.ops:
            if op.name == 'RZ':
                wires.append(op.wires[0])
        return torch.tensor(wires)

    def set_model_weight(self,model,W):
        i = 0
        for para in model.parameters():
            para.data = Parameter(torch.tensor([[W[i].data]]))
            i = i+1


    def init(self, model,prune_ratio):
        self.prune_ratio = prune_ratio
        W = self.get_weight_from_model(model.q_layer)
        self.wires =  self.get_wires_name_from_model(model.q_layer)
        self.ADMM_U = torch.zeros(W.shape).to(self.device)  # add U
        self.ADMM_Z = torch.Tensor(W.shape).to(self.device)  # add Z
    
    def get_sensitive_fixing_parameters(self):
        return self.sensitive_fixing_parameters

    def set_sensitive_fixing_parameters(self,fixing_para):
        self.sensitive_fixing_parameters = fixing_para
        self.sensitive_fixing_parameters = torch.tensor(self.sensitive_fixing_parameters,dtype=torch.float)
    

    def weight_pruning(self, weight):
        pruning_type = self.args.pruning_type
        if pruning_type == 'pruning':
            prune_ratio = self.prune_ratio
            weight = weight.cpu().detach().numpy()  # convert cpu tensor to numpy
            percent = prune_ratio * 100
            weight = torch.tensor(weight)
            fixing_para = torch.zeros_like(weight)
            weight_temp = self.get_fixing_abs(weight,fixing_para)
            percentile = np.percentile(weight_temp, percent)  # get a value for this percentitle
            under_threshold = weight_temp < percentile
            above_threshold = weight_temp >= percentile
            weight[under_threshold] = fixing_para[under_threshold]
            above_threshold = above_threshold.type(torch.float32)  # has to convert bool to float32 for numpy-tensor conversion
            return above_threshold.to(self.device), weight.to(self.device)
        elif pruning_type == 'noise_aware_pruning':
            self.qubit_noise = self.args.qubit_noise.tolist()
            prune_ratio = self.prune_ratio
            weight = weight.cpu().detach().numpy()  # convert cpu tensor to numpy
            percent = prune_ratio * 100
            weight = torch.tensor(weight)
            fixing_para = torch.zeros_like(weight)
            weight_temp = self.get_fixing_abs(weight,fixing_para,self.qubit_noise)
            percentile = np.percentile(weight_temp, percent)  # get a value for this percentitle
            under_threshold = weight_temp < percentile
            above_threshold = weight_temp >= percentile
            weight[under_threshold] = fixing_para[under_threshold]
            above_threshold = above_threshold.type(torch.float32)  # has to convert bool to float32 for numpy-tensor conversion
            return above_threshold.to(self.device), weight.to(self.device)
        elif pruning_type == 'noise_constraining_pruning':
            weight = weight.cpu().detach()
            under_threshold = torch.zeros_like(weight,dtype= torch.bool)
            above_threshold = torch.zeros_like(weight,dtype= torch.bool)
            percent = [40,90,40,40]
            wires_sum = max(self.wires)
            for i in range(wires_sum+1):
                q_weight = weight[self.wires ==i]
                q_weight = torch.tensor(q_weight)
                fixing_para = torch.zeros_like(q_weight)
                weight_temp = self.get_fixing_abs(q_weight,fixing_para)
                percentile = np.percentile(weight_temp, percent[i])  # get a value for this percentitle
                q_under_threshold = weight_temp < percentile
                q_above_threshold = weight_temp >= percentile
                q_weight[q_under_threshold] = fixing_para[q_under_threshold]
                weight[self.wires ==i] = torch.tensor(q_weight).clone()
                under_threshold[self.wires ==i] = q_under_threshold.clone()
                above_threshold[self.wires ==i] = q_above_threshold.clone()
            return above_threshold.to(self.device), weight.to(self.device)

        



    def hard_prune(self,  model):
        """
        hard_pruning, or direct masking
        Args:
             model: contains weight tensors in cuda
    
        """
    
        print("hard pruning")
        W = self.get_weight_from_model(model.q_layer)
        cuda_pruned_weights = None
    
        mask, cuda_pruned_weights = self.weight_pruning(W)  # get sparse model in cuda
    
        W.data = cuda_pruned_weights  # replace the data field in variable
        self.set_model_weight(model,W)
        return mask



    def admm_initialization(self, model):
        if not self.args.admm:
            return
        W = self.get_weight_from_model(model.q_layer)
        _, updated_Z = self.weight_pruning(W)  # Z(k+1) = W(k+1)+U(k)  U(k) is zeros her
        self.ADMM_Z = updated_Z
    
    
    def z_u_update(self, model, epoch,  batch_idx):
        if not self.args.admm:
            return
    
        if epoch != 1 and (epoch - 1) % self.args.admm_epochs == 0 and batch_idx == 0:
    
            Z_prev = None
            W = self.get_weight_from_model(model.q_layer)
            self.ADMM_Z = W.detach() + self.ADMM_U.detach()  # Z(k+1) = W(k+1)+U[        
            _, updated_Z = self.weight_pruning(self.ADMM_Z)  # equivalent to Euclidean Projection
            self.ADMM_Z = updated_Z
            self.ADMM_U = W.detach() - self.ADMM_Z.detach() + self.ADMM_U.detach()  # U(k+1) = W(k+1) - Z(k+1) +U(k)


    def append_admm_loss(self, model, ce_loss):
        '''
        append admm loss to cross_entropy loss
        Args:
            args: configuration parameters
            model: instance to the model class
            ce_loss: the cross entropy loss
        Returns:
            ce_loss(tensor scalar): original cross enropy loss
            admm_loss(dict, name->tensor scalar): a dictionary to show loss for each layer
            ret_loss(scalar): the mixed overall loss
    
        '''
        admm_loss = {}
    
        if self.args.admm:
            W = self.get_weight_from_model(model.q_layer)
            admm_loss = 0.5 * self.rho * (torch.norm(W - self.ADMM_Z + self.ADMM_U, p=2) ** 2)
        mixed_loss = admm_loss
        mixed_loss += ce_loss
        return ce_loss, admm_loss, mixed_loss


    def admm_adjust_learning_rate(self,optimizer, epoch):
        """ (The pytorch learning rate scheduler)
    Set the learning rate to the initial LR decayed by 10 every 30 epochs"""
        """
        For admm, the learning rate change is periodic.
        When epoch is dividable by admm_epoch, the learning rate is reset
        to the original one, and decay every 3 epoch (as the default 
        admm epoch is 9)
    
        """
        admm_epoch = self.args.admm_epochs
        lr = None
        if epoch % admm_epoch == 0:
            lr = self.args.lr
        else:
            admm_epoch_offset = epoch % admm_epoch
    
            admm_step = admm_epoch / 3  # roughly every 1/3 admm_epoch.
    
            lr = self.args.lr * (0.1 ** (admm_epoch_offset // admm_step))
    
        for param_group in optimizer.param_groups:
            param_group['lr'] = lr